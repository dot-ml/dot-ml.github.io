---
---

% Machine Learning - Main Conferences / Journals
@string{uaii = "UAI"}
@string{icml = "ICML"}
@string{nips = "Neural Information Processing Systems (NeurIPS)"}
@string{jmlr = "JMLR"}
@string{iclr = "ICLR"}
@string{iclrws = "ICLR Workshop Track"}
@string{mlsp = "IEEE International Workshop for Machine Learning in Signal Processing"}
@string{aistats = "Artificial Intelligence and Statistics (AISTATS)"}


# Workshops at NIPS, ICML
@string{whi18 = "WHI@ICML"}
@string{fatml = "FAT/ML"}
@string{nips_otml = "NIPS Workshop on Optimal Transport for Machine Learning"}
@string{nips_mlcd = "NIPS Workshop on Machine Learning for Creativity and Design"}
@string{nips_nmlsrl = "NIPS Workshop on Nonparametric Methods for Large Scale Representation Learning"}

% NLP
@string{acl = "ACL"}
@string{conll = "CoNLL"}
@string{emnlp = "EMNLP"}
@string{tacl = "TACL"}

% Vision
@string{cvpr = "CVPR"}
@string{ijcv = "IJCV"}
@string{iccv = "ICCV"}
@string{eccv = "ECCV"}
@string{siggraph = "SIGGRAPH"}

% Theory
@string{tcs = "Theoretical Computer Science"}
@string{soda = "SIAM-ACM Symposium on Discrete Algorithms (SODA)"}
@string{focs = "FOCS)"}
@string{colt = "Conference on Learning Theory (COLT)"}
@string{stoc = "Symposium on Theory of Computing (STOC)"}
@string{icalp = "International Colloquium on Automata, Languages and Programming (ICALP)"}

% Robotics
@string{icra = "IEEE International Conference on Robotics and Automation (ICRA)"}

% Misc AI
@string{jair = "Journal or Artificial Intelligence Research"}
@string{aaai = "AAAI Conference on Artificial Intelligence"}
@string{kdd = "KDD"}
@string{tpami = "TPAMI"}
@string{mathprog = "Mathematical Programming"}
@string{infpl = "Information Processing Letters"}
@string{jacm = "Journal of the ACM"}
@string{tkdd = "ACM Transactions on Knowledge Discovery from Data"}
@string{icalp = "International Colloquium on Automata, Languages, and Programming (ICALP)"}
@string{wads = "Workshop on Algorithms and Data Structures (WADS)"}
@string{wadsnew = "International Symposium on Algorithms and Data Structures (WADS)"}
@string{vldb = "International Conference on Very Large Data Bases (VLDB)"}
@string{wsdm = "ACM International Conference on Web Search and Data Mining (WSDM)"}
@string{acc = "Annual Allerton Conference on Circuit and System Theory"}
@string{www = "Proceedings of the International World Wide Web Conference (WWW)"}
@string{icwsm = "International AAAI Conference on Web and Social Media (ICWSM)"}
@string{hcomp = "HCOMP"}

# More general
@string{plosone = "PLos ONE"}

@string{arxiv = "ArXiv e-prints"}
@string{submission = "In Submission"}

@InProceedings{alvarez-melis2021dataset,
  abbr={ICML},
  selected={true},
  title = 	 {Dataset Dynamics via Gradient Flows in Probability Space},
  author =       {Alvarez-Melis, David and Fusi, Nicol\`o},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {219--230},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/alvarez-melis21a/alvarez-melis21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/alvarez-melis21a.html},
  abstract = 	 {Various machine learning tasks, from generative modeling to domain adaptation, revolve around the concept of dataset transformation and manipulation. While various methods exist for transforming unlabeled datasets, principled methods to do so for labeled (e.g., classification) datasets are missing. In this work, we propose a novel framework for dataset transformation, which we cast as optimization over data-generating joint probability distributions. We approach this class of problems through Wasserstein gradient flows in probability space, and derive practical and efficient particle-based methods for a flexible but well-behaved class of objective functions. Through various experiments, we show that this framework can be used to impose constraints on classification datasets, adapt them for transfer learning, or to re-purpose fixed or black-box models to classify {—}with high accuracy{—} previously unseen datasets.}
}

@inproceedings{alvarez-melis2020geometric,
  abbr={NeurIPS},
  selected={true},
  author = {Alvarez-Melis, David and Fusi, Nicolo},
  booktitle = nips,
  editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages = {21428--21439},
  publisher = {Curran Associates, Inc.},
  title = {Geometric Dataset Distances via Optimal Transport},
  url = {https://proceedings.neurips.cc/paper/2020/file/f52a7b2610fb4d3f74b4106fb80b233d-Paper.pdf},
  volume = {33},
  year = {2020}
}

@incollection{alvarez-melis2018senn,
  abbr={NeurIPS},
  selected={true},
  title = {Towards Robust Interpretability with Self-Explaining Neural Networks},
  author = {Alvarez Melis, David and Jaakkola, Tommi},
  booktitle = {Advances in Neural Information Processing Systems 31},
  editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages = {7786--7795},
  year = {2018},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks.pdf}
}

@InProceedings{alvarez-melis2019towards,
  abbr={AISTATS},
  selected={true},
  title = 	 {Towards Optimal Transport with Global Invariances},
  author = 	 {Alvarez-Melis, David and Jegelka, Stefanie and Jaakkola, Tommi S.},
  booktitle = 	 {Proceedings of Machine Learning Research},
  pages = 	 {1870--1879},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/alvarez-melis19a/alvarez-melis19a.pdf},
  url = 	 {http://proceedings.mlr.press/v89/alvarez-melis19a.html},
  abstract = 	 {Many problems in machine learning involve calculating correspondences between sets of objects, such as point clouds or images. Discrete optimal transport provides a natural and successful approach to such tasks whenever the two sets of objects can be represented in the same space, or at least distances between them can be directly evaluated. Unfortunately neither requirement is likely to hold when object representations are learned from data. Indeed, automatically derived representations such as word embeddings are typically fixed only up to some global transformations, for example, reflection or rotation. As a result, pairwise distances across two such instances are ill-defined without specifying their relative transformation. In this work, we propose a general framework for optimal transport in the presence of latent global transformations. We cast the problem as a joint optimization over transport couplings and transformations chosen from a flexible class of invariances, propose algorithms to solve it, and show promising results in various tasks, including a popular unsupervised word translation benchmark.}
}




% EXAMPLE FIELDS:
%  abbr={AJP},
%  bibtex_show={true},
%  title={The meaning of relativity},
%  author={Einstein, Albert and Taub, AH},
%  journal={American Journal of Physics,},
%  volume={18},
%  number={6},
%  pages={403--404},
%  year={1950},
%  publisher=aps,
%  pdf={example_pdf.pdf},
%  selected={true},
%  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
%  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
